# -*- coding: utf-8 -*-
"""Wakka_Internship_Task2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1k8gx14MqVWVgMe98EyzgJEplBPrIkSp6

### Importing libraries
"""

import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

"""###Creating the fake data"""

np.random.seed(42)

#For each row in matrix X, the code computes the outer product x*xT
#resulting in a square matrix. This matrix is element-wise multiplied by matrix
#C, and the sum of its elements gives the weighted_sum, representing a weighted sum of squared elements in the row vector x.


#The outer product is a matrix where each element is the product of corresponding elements of the two vectors.

#Balanced Gradients: During backpropagation, gradients are computed with respect to the weights. If Y
#has a large scale, the gradients can also be large, leading to unbalanced updates to the weights. Scaling
#Y helps ensure that the gradients are more balanced, which promotes stable weight updates and convergence.




def create_fake_data(num_samples, num_features):
    X = np.random.rand(num_samples, num_features)
    b = np.random.rand(num_features)
    C = np.random.rand(num_features, num_features)

    n = X.shape[0]
    m = X.shape[1]
    Y = np.zeros(n)

    for i in range(n):
        dot_product = np.dot(X[i], b)
        weighted_sum = np.sum(np.outer(X[i], X[i]) * C)
        Y[i] = dot_product + weighted_sum

    Y_normalized = (Y - Y.min()) / (Y.max() - Y.min())





    return X, Y_normalized

X,Y = create_fake_data(10000,15)

"""### Data checking"""

X

Y

X.shape

Y.shape

#random_state is used to shuffle the data before splitting it into training and testing sets.
#splitting the data , 60% for training and the rest 40% for test and validation.

X_train, X_temp, Y_train, Y_temp = train_test_split(X, Y, test_size=0.2, random_state=42)
X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=42)

X_train.shape,Y_train.shape

X_val.shape,Y_val.shape

X_test.shape , Y_test.shape

"""### The model"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras import regularizers
from tensorflow.keras.optimizers import Adam

#Regularization parameter
lmbda = 0

# We use linear activation functoin at the output layer so the model would be able to predict also negative values.

model = Sequential([
    Dense(units=10, input_shape=(X_train.shape[1],), activation='relu', kernel_regularizer=regularizers.l2(lmbda)),
    Dense(units=28, activation='relu', kernel_regularizer=regularizers.l2(lmbda)),
    Dense(units=28, activation='relu', kernel_regularizer=regularizers.l2(lmbda)),
    Dense(units=14, activation='relu', kernel_regularizer=regularizers.l2(lmbda)),
    Dense(units=1, activation='linear', kernel_regularizer=regularizers.l2(lmbda)),
])

model.compile(optimizer=Adam(learning_rate=0.0003),loss = "mean_squared_error")

#These lines create empty lists train_accuracies ,val_accuracies and loss to store training and validation accuracies,and the loss as well.

train_accuracies = []
val_accuracies = []
train_losses = []

# we'll consider a good prediction if |y_train - y_pred| <= 0.01
threshold = 0.01

#This loop iterates through 10 epochs, representing the number of times the entire
#dataset is passed forward and backward through the neural network during training.

num_of_epochs = 60



for epoch in range(num_of_epochs):

    history=model.fit(X_train, Y_train, validation_data=(X_val, Y_val),epochs=1, verbose=2)

    Y_pred_train = model.predict(X_train)
    correct_pred_train = 0

    # Adding the loss history

    train_loss = history.history['loss'][0]
    train_losses.append(train_loss)
    ##


    for i in range(len(Y_train)):
        if np.abs(Y_train[i] - Y_pred_train[i]) <= threshold:
            correct_pred_train += 1
    train_accuracy = correct_pred_train / len(Y_train)
    train_accuracies.append(train_accuracy)

    Y_pred_val = model.predict(X_val)
    correct_pred_val = 0

    for i in range(len(Y_val)):
        if np.abs(Y_val[i] - Y_pred_val[i]) <= threshold:
            correct_pred_val += 1
    val_accuracy = correct_pred_val / len(Y_val)
    val_accuracies.append(val_accuracy)

    print(f"Epoch {epoch + 1}/{num_of_epochs} - "
      f"Training Accuracy: {train_accuracy:.2%} - Validation Accuracy: {val_accuracy:.2%}")


#Plotting accuracies and loss history.

plt.plot(train_accuracies, label='Training Accuracy')
plt.plot(val_accuracies, label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

plt.plot(train_losses, label='Training Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

"""### Testing out model on unseen data"""

test_predictions = model.predict(X_test)
correct_pred_test = 0

for i in range(len(Y_test)):
    if np.abs(Y_test[i] - test_predictions[i]) <= threshold:
        correct_pred_test += 1

total_examples_test = len(Y_test)
test_accuracy = correct_pred_test / total_examples_test

print(f"Test Accuracy: {test_accuracy:.2%}")

