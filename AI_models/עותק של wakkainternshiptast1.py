# -*- coding: utf-8 -*-
"""WakkaInternshipTast1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1N6RLDOlmn96eoGmfOyv8h9X2GY0QVLkM

### Libraries
"""

import numpy as np
import tensorflow as tf
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

#Determines the randomness

np.random.seed(42)

"""### Generating the fake data"""

def generate_fake_data(num_samples, num_features):
  X_train = np.random.rand(num_samples, num_features)
  Coefficients  = np.random.rand(num_features)
  Y_train = np.dot(X_train,Coefficients)
  return X_train, Y_train

# We have 2000 examples and 10 features

X_train, Y_train = generate_fake_data(2000,10)

"""### Data checking"""

X_train

Y_train

X_train.shape

Y_train.shape

#random_state is used to shuffle the data before splitting it into training and testing sets.

X_train, X_temp, Y_train, Y_temp = train_test_split(X_train, Y_train, test_size=0.4, random_state=42)
X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=42)

X_train.shape,Y_train.shape

X_val.shape,Y_val.shape

X_test.shape , Y_test.shape

"""### The Model


"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras import regularizers
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.optimizers import Adam

#regularization parameter
lmbda = 0.001

model = Sequential([
    Dense(units=10, input_shape=(X_train.shape[1],), activation='relu', kernel_regularizer=regularizers.l2(lmbda)),
    Dense(units=10, activation='relu', kernel_regularizer=regularizers.l2(lmbda)),
    Dense(units=1, activation='linear', kernel_regularizer=regularizers.l2(lmbda)),
])

model.compile(optimizer=Adam(learning_rate=0.05), loss='mean_squared_error')

#These lines create empty lists train_accuracies and val_accuracies to store training and validation accuracies.

train_accuracies = []
val_accuracies = []

# we'll consider a good prediction if |y_train - y_pred| <= 0.01
threshold = 0.01

#This loop iterates through 10 epochs, representing the number of times the entire
#dataset is passed forward and backward through the neural network during training.

num_of_epochs = 10

for epoch in range(num_of_epochs):

    model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=1, verbose=1)

    Y_pred_train = model.predict(X_train)
    correct_pred_train = 0

    for i in range(len(Y_train)):
        if np.abs(Y_train[i] - Y_pred_train[i]) <= threshold:
            correct_pred_train += 1
    train_accuracy = correct_pred_train / len(Y_train)
    train_accuracies.append(train_accuracy)

    Y_pred_val = model.predict(X_val)
    correct_pred_val = 0

    for i in range(len(Y_val)):
        if np.abs(Y_val[i] - Y_pred_val[i]) <= threshold:
            correct_pred_val += 1
    val_accuracy = correct_pred_val / len(Y_val)
    val_accuracies.append(val_accuracy)

    print(f"Epoch {epoch + 1}/{num_of_epochs} - Training Accuracy: {train_accuracy:.2%} - Validation Accuracy: {val_accuracy:.2%}")

plt.plot(train_accuracies, label='Training Accuracy')
plt.plot(val_accuracies, label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

test_predictions = model.predict(X_test)
correct_pred_test = 0

for i in range(len(Y_test)):
    if np.abs(Y_test[i] - test_predictions[i]) <= threshold:
        correct_pred_test += 1

total_examples_test = len(Y_test)
test_accuracy = correct_pred_test / total_examples_test

print(f"Test Accuracy: {test_accuracy:.2%}")